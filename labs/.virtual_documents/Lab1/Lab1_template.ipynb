








import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# URL to the Abalone dataset 
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"

# Define column names based on the UCI repository documentation
column_names = [
    "Sex", "Length", "Diameter", "Height", 
    "Whole_weight", "Shucked_weight", "Viscera_weight", 
    "Shell_weight", "Rings"
]

# Load the dataset
df = pd.read_csv(url, names=column_names)

# Display the first few rows
df.head()





# Explore and Visualize data 
# ex: scatterplot, correlation matrix

# Plot the distribution of Rings (Age Indicator)
plt.figure(figsize=(10, 6))
sns.histplot(df['Rings'], bins=15, kde=True)
plt.title('Distribution of Abalone Age (Rings)')
plt.xlabel('Rings (Age Indicator)')
plt.ylabel('Frequency')
plt.show()








# Prepare the Data for Machine Learning 
# Preprocessing: handling missing values, feature scaling, splitting data into training/test 
# train_test_split()

X = df[['Shucked_weight']]
Y = df[['Whole_weight']]

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.33, random_state=42)








# Select and Train Model
# Import and create regression model using LinearRegression() from scikit-learn
# Train the model on training data with .fit()

# Initialize and train the model
model = LinearRegression()
model.fit(X_train, Y_train)





#Evaluate the Model 
#Predict target variable on the test set and calculate evaluation metrics using scikit-learn's mean_squared_error adn r2_score
#Visualize predictions vs. actual values

from sklearn.metrics import mean_squared_error, r2_score

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

# Plot predictions vs actual
plt.scatter(Y_test, Y_pred)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')
plt.show()









#Fine-tune the Model 
#Use cross_val_score() to fine-tune the model

from sklearn.model_selection import cross_val_score

# Perform cross-validation
scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-validation scores: {-scores}")









# Transform features for polynomial regression 

from sklearn.preprocessing import PolynomialFeatures

# Transform features to include polynomial terms (degree 2 for quadratic terms)
poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

# View the transformed feature set (for insight)
print(X_poly_train)





# Train the model on polynomial features 
poly_model = LinearRegression()
poly_model.fit(X_poly_train, Y_train)





# Evaluate the polynomial regression model
# Make predictions using the polynomial model
Y_poly_pred = poly_model.predict(X_poly_test)

# Evaluate the polynomial model
poly_mse = mean_squared_error(Y_test, Y_poly_pred)
poly_r2 = r2_score(Y_test, Y_poly_pred)

print(f"Polynomial Regression Mean Squared Error: {poly_mse}")
print(f"Polynomial Regression R² Score: {poly_r2}")

# Plot predictions vs actual
plt.scatter(Y_test, Y_poly_pred)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Polynomial Regression: Actual vs Predicted')
plt.show()















