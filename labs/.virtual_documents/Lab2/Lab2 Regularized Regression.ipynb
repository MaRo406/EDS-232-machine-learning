


import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import kagglehub

# URL to the Abalone dataset 
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"

# Define column names based on the UCI repository documentation
column_names = [
    "Sex", "Length", "Diameter", "Height", 
    "Whole_weight", "Shucked_weight", "Viscera_weight", 
    "Shell_weight", "Rings"
]

# Load the dataset
df = pd.read_csv(url, names=column_names)

# Display the first few rows
df.head()
# Explore the data
print(df.info())
print(df.head())






# Explore and Visualize data 
# ex: scatterplot, correlation matrix

# Plot the distribution of Rings (Age Indicator)
plt.figure(figsize=(10, 6))
sns.histplot(df['Rings'], bins=15, kde=True)
plt.title('Distribution of Abalone Age (Rings)')
plt.xlabel('Rings (Age Indicator)')
plt.ylabel('Frequency')
plt.show()





# Define predictors and outcome

X = df[['Shucked_weight']]
Y = df[['Whole_weight']]

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.33, random_state=42)


# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)






from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import RidgeCV

# Standardize the predictors
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ridge regression with varying alphas
alphas = np.logspace(-4, 4, 50) # reduced alphas for run time for time being
ridge = RidgeCV(alphas=alphas,  store_cv_results=True) # removing cv = , can't have both cv = and store_cv_results = True

ridge.fit(X_train_scaled, y_train)


n_alphas = 10
alphas = np.logspace(-10, -2, n_alphas)

coefs = []
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X_train_scaled, y_train)
    coefs.append(ridge.coef_)


# Coefficients plot
plt.figure(figsize=(10, 6))
plt.plot(ridge.alphas, ridge.cv_results_.mean(axis=0).flatten(), label="CV Error") # flattened cv results and updated to cv_results_ instead of cv_values_
plt.xscale("log")
plt.xlabel("Lambda (Alpha)")
plt.ylabel("Cross-Validated MSE")
plt.title("Ridge Regression Coefficients vs. Lambda")
plt.legend()
plt.show()





X_train_scaled.shape


y_train.values.ravel()


from sklearn.linear_model import LassoCV

# Lasso regression with cross-validation
lasso = LassoCV(alphas=alphas, cv=10, random_state=42)
lasso.fit(X_train_scaled,y_train.values.ravel()) # included .ravel to make 1D to avoid warning message 

# Plot Lasso results
plt.figure(figsize=(10, 6))
plt.plot(np.log10(lasso.alphas_), lasso.mse_path_.mean(axis=1), label="CV Error")
plt.xlabel("Log(Lambda)")
plt.ylabel("Cross-Validated MSE")
plt.title("Lasso Regression CV Results")
plt.legend()
plt.show()






# Ridge results
ridge_min_mse = ridge.cv_results_.min() # changing cv_values_ to cv_results
ridge_best_alpha = ridge.alpha_

# Lasso results
lasso_min_mse = lasso.mse_path_.min()
lasso_best_alpha = lasso.alpha_

print(f"Ridge - Minimum MSE: {ridge_min_mse}, Best Alpha: {ridge_best_alpha}")
print(f"Lasso - Minimum MSE: {lasso_min_mse}, Best Alpha: {lasso_best_alpha}")






# One-standard-error rule for Lasso
lasso_best_alpha_1se = lasso.alphas_[np.where(
    lasso.mse_path_.mean(axis=1) <= (lasso.mse_path_.mean(axis=1).min() + lasso.mse_path_.std(axis=1).mean())
)[0][0]]

lasso_1se_model = Lasso(alpha=lasso_best_alpha_1se)
lasso_1se_model.fit(X_train_scaled, y_train)

print(f"Lasso 1-SE Rule Alpha: {lasso_best_alpha_1se}")
print(f"Number of Predictors in Lasso (1-SE): {np.sum(lasso_1se_model.coef_ != 0)}")







